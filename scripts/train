#!/usr/bin/env python
from __future__ import absolute_import, division, print_function
from depth_correction.depth_cloud import DepthCloud
from depth_correction.filters import filter_depth, filter_eigenvalue, filter_grid
from depth_correction.loss import min_eigval_loss
from depth_correction.model import *
from depth_correction.utils import timer, timing
import importlib
import numpy as np
import sys
import torch
from torch.utils.tensorboard import SummaryWriter

from rospkg import RosPack
pkg_dir = RosPack().get_path('depth_correction')
# pkg_dir = '/home/petrito1/workspace/depth_correction'
print(pkg_dir)

# MODEL_TYPE = 'Linear'
# MODEL_TYPE = 'Polynomial'
MODEL_TYPE = 'ScaledPolynomial'
# MODEL_TYPE = 'InvCos'
# MODEL_TYPE = 'ScaledInvCos'

N_OPT_ITERS = 100
LR = 1e-2
# LR = 1e-3
# LR = 1e-4
# LR = 1e-6
SHOW_RESULTS = False

DATASET = 'asl_laser'
# dataset = 'utias_3dmap'
# dataset = 'chilean_mine'
# dataset = 'esa_robotics'
print(DATASET)

dtype = np.float64
# dtype = torch.float64
device = torch.device('cpu')
# device = torch.device('cuda')

min_depth = 1.0
max_depth = 15.0
# grid_res = 0.05
grid_res = 0.1

k = None
# r = 0.15
r = 0.2
# max_eig_0 = 0.005**2
# max_eig_0 = 0.01**2
max_eig_0 = 0.02**2
min_eig_1 = 0.05**2
step = 3
log_filters = False
plot_period = 10

# from eval('data.%s' % dataset) import Dataset, dataset_names
# eval('from data.%s import Dataset, dataset_names' % dataset)
imported_module = importlib.import_module("data.%s" % DATASET)
Dataset = getattr(imported_module, "Dataset")
dataset_names = getattr(imported_module, "dataset_names")
print('Using %s datasets %s.' % (DATASET, ', '.join(dataset_names)))

# train_names = 'eth',
train_names = 'apartment', 'eth',
val_names = 'stairs', 'gazebo_winter',
print('Training set: %s.' % ', '.join(train_names))
print('Validation set: %s.' % ', '.join(val_names))

@timing
def filtered_cloud(cloud):
    cloud = filter_depth(cloud, min=min_depth, max=max_depth, log=log_filters)
    cloud = filter_grid(cloud, grid_res=grid_res, keep='random', log=log_filters)
    return cloud


@timing
def filtered_clouds(ds: Dataset):
    clouds = []
    poses = []
    for i, (cloud, pose) in enumerate(ds):
        cloud = filtered_cloud(cloud)
        clouds.append(cloud)
        poses.append(pose)
    return list(zip(clouds, poses))


@timing
def local_feature_cloud(cloud):
    # Convert to depth cloud and transform.
    cloud = DepthCloud.from_structured_array(cloud, dtype=dtype)
    cloud = cloud.to(device=device)
    # Find/update neighbors and estimate all features.
    cloud.update_all(k=k, r=r)
    # Select planar regions to correct as in prediction phase.
    mask = filter_eigenvalue(cloud, 0, max=max_eig_0, only_mask=True, log=log_filters)
    mask = mask & filter_eigenvalue(cloud, 1, min=min_eig_1, only_mask=True, log=log_filters)
    cloud.mask = mask
    return cloud


@timing
def local_feature_clouds(ds):
    clouds = []
    poses = []
    for i, (cloud, pose) in enumerate(ds):
        cloud = local_feature_cloud(cloud)
        pose = torch.as_tensor(pose, device=device)
        clouds.append(cloud)
        poses.append(pose)
    return list(zip(clouds, poses))


def global_cloud(clouds: (list, tuple),
                 model: BaseModel,
                 poses: torch.Tensor):
    """Create global cloud with corrected depth.

    :param clouds: Filtered local features clouds.
    :param model: Depth correction model, directly applicable to clouds.
    :param poses: N-by-4-by-4 pose tensor to transform clouds to global frame.
    :return: Global cloud with corrected depth.
    """
    transformed_clouds = []
    for i, cloud in enumerate(clouds):
        cloud = model(cloud)
        cloud = cloud.transform(poses[i])
        transformed_clouds.append(cloud)
    cloud = DepthCloud.concatenate(transformed_clouds, True)
    # cloud.visualize(colors='z')
    # cloud.visualize(colors='inc_angles')
    return cloud


def global_clouds(clouds, model, poses):
    ret = []
    for c, p in zip(clouds, poses):
        cloud = global_cloud(c, model, p)
        ret.append(cloud)
    return ret


def main():
    # Cloud needs to retain neighbors, weights, and mask from previous
    # iterations.
    # Depth correction is applied based on local cloud statistics.
    # Loss is computed based on global cloud statistics.

    train_clouds = []
    train_poses = []
    # Pose corrections :3 elements axis-angle, 3 translation.
    train_pose_deltas = []
    train_neighbors = [None] * len(train_names)
    train_masks = [None] * len(train_names)
    for name in train_names:
        clouds = []
        poses = []
        for cloud, pose in Dataset(name)[::step]:
            cloud = filtered_cloud(cloud)
            cloud = local_feature_cloud(cloud)
            # If poses are not optimized, depth can be corrected on global
            # feature clouds.
            # If poses are to be optimized, depth can be corrected on local
            # clouds and these can then be transformed to global cloud.
            clouds.append(cloud)
            poses.append(pose)
        train_clouds.append(clouds)
        poses = np.stack(poses).astype(dtype=dtype)
        poses = torch.as_tensor(poses, device=device)
        train_poses.append(poses)
        pose_deltas = torch.zeros((poses.shape[0], 6))
        train_pose_deltas.append(pose_deltas)

    val_clouds = []
    val_poses = []
    val_neighbors = [None] * len(val_names)
    val_masks = [None] * len(val_names)
    for name in val_names:
        clouds = []
        poses = []
        for cloud, pose in Dataset(name)[::step]:
            cloud = filtered_cloud(cloud)
            cloud = local_feature_cloud(cloud)
            clouds.append(cloud)
            poses.append(pose)
        val_clouds.append(clouds)
        poses = np.stack(poses).astype(dtype=dtype)
        poses = torch.as_tensor(poses, device=device)
        val_poses.append(poses)

    # Create model
    model = eval(MODEL_TYPE)(device=device)
    print(model)

    # Initialize optimizer
    # optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)

    writer = SummaryWriter('%s/config/tb_runs/model_%s_lr_%f_%s_%f'
                           % (pkg_dir, MODEL_TYPE, LR, DATASET, timer()))

    min_loss = np.inf
    for it in range(N_OPT_ITERS):
        optimizer.zero_grad()

        # Training

        # TODO: Allow optimizing pose deltas.
        # if train_pose_deltas is None:
        #     poses = train_poses
        # else:
        #     # TODO: Convert pose deltas to matrices
        #     pass
        clouds = global_clouds(train_clouds, model, train_poses)
        for i in range(len(clouds)):
            cloud = clouds[i]
            if train_neighbors[i] is None:
                cloud.update_all(k=k, r=r)
                train_neighbors[i] = cloud.neighbors, cloud.weights
                mask = filter_eigenvalue(cloud, 0, max=max_eig_0, only_mask=True, log=log_filters)
                mask = mask & filter_eigenvalue(cloud, 1, min=min_eig_1, only_mask=True, log=log_filters)
                print('Training on %.3f = %i / %i points.'
                      % (mask.float().mean().item(), mask.sum().item(), mask.numel()))
                train_masks[i] = mask
            else:
                cloud.neighbors, cloud.weights = train_neighbors[i]
                cloud.update_all(k=k, r=r, keep_neighbors=True)
            clouds[i] = cloud

        train_loss, _ = min_eigval_loss(clouds, mask=train_masks)
        writer.add_scalar("min_eigval_loss/train", train_loss, it)

        # Validation

        clouds = global_clouds(val_clouds, model, val_poses)
        for i in range(len(clouds)):
            cloud = clouds[i]
            if val_neighbors[i] is None:
                cloud.update_all(k=k, r=r)
                val_neighbors[i] = cloud.neighbors, cloud.weights
                mask = filter_eigenvalue(cloud, 0, max=max_eig_0, only_mask=True, log=log_filters)
                mask = mask & filter_eigenvalue(cloud, 1, min=min_eig_1, only_mask=True, log=log_filters)
                print('Validating on %.3f = %i / %i points.'
                      % (mask.float().mean().item(), mask.sum().item(), mask.numel()))
                val_masks[i] = mask
            else:
                cloud.neighbors, cloud.weights = val_neighbors[i]
                cloud.update_all(k=k, r=r, keep_neighbors=True)
            clouds[i] = cloud

        val_loss, _ = min_eigval_loss(clouds, mask=val_masks)
        writer.add_scalar("min_eigval_loss/val", val_loss, it)

        # if SHOW_RESULTS and i % plot_period == 0:
        #     for dc in clouds:
        #         dc.visualize(colors='inc_angles')
        #         dc.visualize(colors='min_eigval')
        #     dc_loss.visualize(colors='loss')

        if val_loss.item() < min_loss:
            saved = True
            min_loss = val_loss.item()
            torch.save(model.state_dict(),
                       '%s/config/weights/%s_train_%s_val_%s_r%.2f_eig_%.4f_%.4f_min_eigval_loss_%.9f.pth'
                       % (pkg_dir, MODEL_TYPE, ','.join(train_names), ','.join(val_names),
                          r, max_eig_0, min_eig_1, val_loss.item()))
        else:
            saved = False

        print('It. %i: training loss: %.9f, validation: %.9f. Model %s %s.'
              % (it, train_loss.item(), val_loss.item(), model, 'saved' if saved else 'not saved'))

        # Optimization step
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()

    # del ds, datasets
    writer.flush()
    writer.close()


if __name__ == '__main__':
    main()
