#!/usr/bin/env python

from __future__ import absolute_import, division, print_function

import os
from depth_correction.filters import *
from depth_correction.model import *
from depth_correction.utils import timing
from depth_correction.loss import min_eigval_loss
from data.asl_laser import Dataset, dataset_names
from ros_numpy import msgify, numpify
import rospy
from rospkg import RosPack
from sensor_msgs.msg import PointCloud2
import torch
from torch.utils.tensorboard import SummaryWriter
from timeit import default_timer as timer


def load_model(class_name, state_dict, device=torch.device('cpu')):
    if isinstance(device, str):
        device = torch.device(device)
    Class = eval(class_name)
    model = Class()
    assert isinstance(model, BaseModel)
    if state_dict and os.path.exists(state_dict):
        rospy.loginfo('Using %s model on %s device with state %s.', class_name, device, state_dict)
        model.load_state_dict(torch.load(state_dict))
    else:
        rospy.logwarn('Using %s model on %s device with initial state.', class_name, device)
    model.train()
    model.to(device)
    return model


def cloud_to_ros_msg(dc, frame_id, stamp):
    pc_msg = msgify(PointCloud2, dc.to_structured_array())
    pc_msg.header.frame_id = frame_id
    pc_msg.header.stamp = stamp
    return pc_msg


class Trainer(object):
    def __init__(self):
        self.device = torch.device(rospy.get_param('~device', 'cpu'))
        self.model = load_model(rospy.get_param('~model/class'),
                                rospy.get_param('~model/state_dict', None),
                                self.device)
        self.lr = rospy.get_param('~lr', 0.001)
        self.train_names = rospy.get_param('train_seqs', ['apartment', 'eth'])
        self.val_names = rospy.get_param('val_seqs', ['stairs', 'gazebo_winter'])
        self.step = rospy.get_param('~step', 3)
        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, nesterov=True)
        self.n_iters = rospy.get_param('~n_iters', 100)

        self.r_nn = rospy.get_param('~r_nn', 0.20)
        self.min_depth = rospy.get_param('~min_depth', 1.0)
        self.max_depth = rospy.get_param('~max_depth', 10.0)
        self.grid_res = rospy.get_param('~grid_res', 0.10)
        self.log_filters = rospy.get_param('~log_filters', False)
        self.max_eig_0 = rospy.get_param('~max_eig_0', 0.02 ** 2)
        self.min_eig_1 = rospy.get_param('~max_eig_1', 0.05 ** 2)
        self.dtype = np.float64
        self.train_neighbors = None

        self.path = RosPack().get_path('depth_correction')
        self.tb_writer = SummaryWriter(os.path.join(self.path,
                                                    'config/tb_runs/model_%s_lr_%f_ASL_laser_%f' %
                                                    (rospy.get_param('~model/class'), self.lr, timer())))

        self.world_frame = rospy.get_param('~world_frame', 'world')

    def filtered_cloud(self, cloud):
        cloud = filter_depth(cloud, min=self.min_depth, max=self.max_depth, log=self.log_filters)
        cloud = filter_grid(cloud, grid_res=self.grid_res, keep='random', log=self.log_filters)
        return cloud

    def local_feature_cloud(self, cloud):
        # Convert to depth cloud and transform.
        cloud = DepthCloud.from_structured_array(cloud, dtype=self.dtype)
        cloud = cloud.to(device=self.device)
        # Find/update neighbors and estimate all features.
        cloud.update_all(r=self.r_nn)
        # Select planar regions to correct as in prediction phase.
        mask = filter_eigenvalue(cloud, 0, max=self.max_eig_0, only_mask=True, log=self.log_filters)
        mask = mask & filter_eigenvalue(cloud, 1, min=self.min_eig_1, only_mask=True, log=self.log_filters)
        cloud.mask = mask
        return cloud

    def global_cloud(self, clouds, poses: torch.Tensor) -> DepthCloud:
        assert poses.dim() == 3
        assert poses.shape[1:] == (4, 4)
        transformed_clouds = []
        for i, dc in enumerate(clouds):
            dc = self.model(dc)
            pose = poses[i]
            dc = dc.transform(pose)
            transformed_clouds.append(dc)
        cloud = DepthCloud.concatenate(transformed_clouds, True)
        return cloud

    def global_clouds(self, clouds, poses):
        ret = []
        for c, p in zip(clouds, poses):
            cloud = self.global_cloud(c, p)
            ret.append(cloud)
        return ret

    def train(self):
        train_clouds = []
        train_poses = []
        # Pose corrections :3 elements axis-angle, 3 translation.
        train_pose_deltas = []
        train_neighbors = [None] * len(self.train_names)
        train_masks = [None] * len(self.train_names)
        for name in self.train_names:
            clouds = []
            poses = []
            for cloud, pose in Dataset(name)[::self.step]:
                cloud = self.filtered_cloud(cloud)
                cloud = self.local_feature_cloud(cloud)
                # If poses are not optimized, depth can be corrected on global
                # feature clouds.
                # If poses are to be optimized, depth can be corrected on local
                # clouds and these can then be transformed to global cloud.
                clouds.append(cloud)
                poses.append(pose)
            train_clouds.append(clouds)
            poses = np.stack(poses).astype(dtype=self.dtype)
            poses = torch.as_tensor(poses, device=self.device)
            train_poses.append(poses)
            pose_deltas = torch.zeros((poses.shape[0], 6))
            train_pose_deltas.append(pose_deltas)

        val_clouds = []
        val_poses = []
        val_neighbors = [None] * len(self.val_names)
        val_masks = [None] * len(self.val_names)
        for name in self.val_names:
            clouds = []
            poses = []
            for cloud, pose in Dataset(name)[::self.step]:
                cloud = self.filtered_cloud(cloud)
                cloud = self.local_feature_cloud(cloud)
                clouds.append(cloud)
                poses.append(pose)
            val_clouds.append(clouds)
            poses = np.stack(poses).astype(dtype=self.dtype)
            poses = torch.as_tensor(poses, device=self.device)
            val_poses.append(poses)

        min_loss = np.inf
        for it in range(self.n_iters):

            if rospy.is_shutdown():
                break

            # Training

            # TODO: Allow optimizing pose deltas.
            # if train_pose_deltas is None:
            #     poses = train_poses
            # else:
            #     # TODO: Convert pose deltas to matrices
            #     pass
            clouds = self.global_clouds(train_clouds, train_poses)
            for i in range(len(clouds)):
                cloud = clouds[i]
                if train_neighbors[i] is None:
                    cloud.update_all(r=self.r_nn)
                    train_neighbors[i] = cloud.neighbors, cloud.weights
                    mask = filter_eigenvalue(cloud, 0, max=self.max_eig_0, only_mask=True, log=self.log_filters)
                    mask = mask & filter_eigenvalue(cloud, 1, min=self.min_eig_1, only_mask=True, log=self.log_filters)
                    rospy.loginfo('Training on %.3f = %i / %i points.'
                                  % (mask.float().mean().item(), mask.sum().item(), mask.numel()))
                    train_masks[i] = mask
                else:
                    cloud.neighbors, cloud.weights = train_neighbors[i]
                    cloud.update_all(r=self.r_nn, keep_neighbors=True)
                clouds[i] = cloud

            train_loss, _ = min_eigval_loss(clouds, mask=train_masks)
            self.tb_writer.add_scalar("min_eigval_loss/train", train_loss, it)

            # Validation

            clouds = self.global_clouds(val_clouds, val_poses)
            for i in range(len(clouds)):
                cloud = clouds[i]
                if val_neighbors[i] is None:
                    cloud.update_all(r=self.r_nn)
                    val_neighbors[i] = cloud.neighbors, cloud.weights
                    mask = filter_eigenvalue(cloud, 0, max=self.max_eig_0, only_mask=True, log=self.log_filters)
                    mask = mask & filter_eigenvalue(cloud, 1, min=self.min_eig_1, only_mask=True, log=self.log_filters)
                    rospy.loginfo('Validating on %.3f = %i / %i points.'
                                  % (mask.float().mean().item(), mask.sum().item(), mask.numel()))
                    val_masks[i] = mask
                else:
                    cloud.neighbors, cloud.weights = val_neighbors[i]
                    cloud.update_all(r=self.r_nn, keep_neighbors=True)
                clouds[i] = cloud

            val_loss, _ = min_eigval_loss(clouds, mask=val_masks)
            self.tb_writer.add_scalar("min_eigval_loss/val", val_loss, it)

            if val_loss.item() < min_loss:
                saved = True
                min_loss = val_loss.item()
                model_name = '%s_train_%s_val_%s_r%.2f_eig_%.4f_%.4f_min_eigval_loss_%.9f.pth' %\
                             (rospy.get_param('~model/class'), self.train_names, self.val_names, self.r_nn,
                              self.max_eig_0, self.min_eig_1, val_loss.item())
                torch.save(self.model.state_dict(), os.path.join(self.path, 'config/weights', model_name))
            else:
                saved = False

            rospy.loginfo('It. %i: training loss: %.9f, validation: %.9f. Model %s %s.'
                          % (it, train_loss.item(), val_loss.item(), self.model, 'saved' if saved else 'not saved'))

            # Optimization step
            self.optimizer.zero_grad()
            train_loss.backward()
            self.optimizer.step()


def main():
    rospy.init_node('depth_correction_trainer', log_level=rospy.INFO)
    trainer = Trainer()
    trainer.train()
    rospy.spin()


if __name__ == '__main__':
    main()
