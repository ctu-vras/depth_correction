#! /usr/bin/env python
import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm
from data.depth_correction import Dataset, dataset_names
from depth_correction.depth_cloud import DepthCloud
from depth_correction.model import ScaledPolynomial
from depth_correction.preproc import filtered_cloud
from depth_correction.config import Config
from depth_correction.loss import point_to_plane_dist

# in this example we use collected indoor dataset
ds = Dataset(name=dataset_names[0][:24] + '_step_1')

# setup data and optimization parameters
cfg = Config()
cfg.lr = 0.001
cfg.device = 'cuda'

plt.figure()


best_weights = []

n_train_samples = len(ds) - 1
# n_train_samples = 10
for ds_id in range(n_train_samples):
    # the two neighboring scans are used
    points1_struct, pose1 = ds[ds_id]
    points2_struct, pose2 = ds[ds_id + 1]

    # construct depth cloud objects from points
    cloud1 = DepthCloud.from_structured_array(points1_struct).to(cfg.device)
    cloud2 = DepthCloud.from_structured_array(points2_struct).to(cfg.device)

    # apply grid and depth filters to clouds
    cloud1 = filtered_cloud(cloud1, cfg)
    cloud2 = filtered_cloud(cloud2, cfg)

    # transform point clouds to the same world coordinate frame
    cloud1 = cloud1.transform(torch.as_tensor(pose1))
    cloud2 = cloud2.transform(torch.as_tensor(pose2))

    # compute cloud features necessary for optimization (like normals and incidence angles)
    cloud1.update_all(r=cfg.nn_r)
    cloud2.update_all(r=cfg.nn_r)

    # run optimization loop
    losses = []
    weights = np.linspace(-1e-2, 1e-2, 200)
    for w in tqdm(weights):
        model = ScaledPolynomial(w=[w], exponent=[6.0], device=cfg.device)
        cloud1_corr = model(cloud1)
        cloud2_corr = model(cloud2)

        cloud1_corr.update_points()
        cloud2_corr.update_points()

        loss = point_to_plane_dist(clouds=[cloud1_corr, cloud2_corr])

        losses.append(loss.item())

    best_weight = weights[np.argmin(losses)]
    best_weights.append(best_weight)

    plt.plot(weights, np.asarray(losses) - losses[0], label='(%i, %i)' % (ds_id, ds_id + 1))
    plt.pause(0.01)
    plt.draw()
    plt.grid(visible=True)
    plt.legend()

best_weights = np.asarray(best_weights)
mean_weight = np.mean(best_weights)
std_weight = np.std(best_weights)
gaussian = np.exp(- 0.5 * (best_weights - mean_weight) ** 2 / (std_weight ** 2)) #  / (std_weight * np.sqrt(2 * np.pi))

plt.figure()
# plt.title('Found weights for different training sequences')
# plt.plot(list(range(n_train_samples)), best_weights, 'x')
plt.plot(best_weights, gaussian, 'o')
plt.grid(visible=True)
# plt.xlabel('train cloud id')
# plt.ylabel('best train weight')

plt.show()
